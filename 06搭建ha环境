1. 基础环境
    1. 增加NNs的ssh免密
    2. 应用搭建
        1. zookeeper
        2. 格式化NN
        3. 格式化ZK
    3. 启动集群

配置：
    core-site.xml
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://mycluster</value>
        </property>
        <property>
            <name>ha.zookeeper.quorm</name>
            <value>node02:2181,node03:2181,node04:2181/value>
        </property>

    hdfs-site.xml
        <property>
            <name>dfs.nameservices</name>
            <value>mycluster</value>
        </property>
        <property>
            <name>dfs.ha.namenode.mycluster</name>
            <value>nn1,nn2</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn1<name>
            <value>node01:8020</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn2</name>
            <value>node02:8020</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.mycluster.nn1</name>
            <value>node01:50070</value>
        </property>
        <property>
        </property>
            <name>dfs.namenode.http-address.mycluster.nn2</name>
            <value>node02:50070</value>
        </property>

        <!-- edits共享数据目录 实际是在配置journal node 在哪些机器 -->
        <property>
            <name>dfs.namenode.shared.edits.dir</name>
            <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
        </property>
        <property>
            <name>dfs.journalnode.edits.dir</name>
            <value>/var/bigdata/hadoop/ha/dfs/jn<value>
        </property>

        <!-- 发送ha切换指令的时候 用该配置类指定谁是namenode，谁是standby -->
        <property>
            <name>dfs.client.failover.proxy.provider.mycluster</name>
            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>

        <!-- 怎么发送信号，通过以下两个配置实现 -->
        <property>
            <name>dfs.ha.fencing.methods</name>
            <value>sshfence</value>
        </property>
        <property>
            <name>dfs.ha.fencing.ssh.private-key-files</name>
            <value>/home/exampleuser/.ssh/id_rsa</value>
        </property>
        <property>
            <name>dfs.ha.automic-failover.enabled</name>
            <value>true</value>
        </property>

免密可以用于node01 和 node02免密登录，也可以用于zkfc，控制namenode上下线


hdfs zkfc -formatZK
This will create a znode in Zookeeper inside of which the automatic failover system stores its data

After all of the necessary configuration options have been set, you must start start the JournalNode daemons on the set of machines where they will run.
This can be done by running the command:（启动 journalnode)
hadoop-daemon.sh start journalnode

之后开始格式化:
选择其中一台namenode节点格式化：
hdfs namenode -format
另外一个namenode执行如下命令：(置其为standby）
hdfs namenode -bootstrapStandby


流程：
    基础设施：
        ssh免密：
            1）启动start-dfs.sh脚本的机器需要将公钥分发给别的节点
            2）在HA模式下，每一个NN身边会启动ZKFC，ZKFC会用免密的方式控制自己和其他NN节点的NN状态
    应用搭建：
        HA 依赖 ZK 搭建ZK集群
        修改hadoop的配置文件，并集群同步
    初始化启动：
        1） 先启动JN hadoop-daemon.sh start journalnode     (对应journalnode有的节点（在这里是node01,node02,node03)都需要执行
        2)  选择一个NN做格式化：hdfs namenode format  // 只有第一次搭建做，以后不用做
        3） 启动这个格式化的NN，以备另外一台同步：hadoop-daemon.sh start namenode
        4） 在另外一台机器中： hdfs namenode -bootstrapStandby
        5)  格式化zk：hdfs zkfc -formatZK   // 只有第一次搭建做，以后不用做
        6）  start-dfs.sh
    使用：


------ 实操
    1）停止之前的集群
    2）免密：node01,node02
        node02:
            cd ~/.ssh
            ssh-keygen -t dsa -P '' -f ./id_dsa
            cat id_dsa.pub >> authorized_keys
            scp ./id_dsa.pub node01:`pwd` /node02.pub
        node01:
            cd ~/.ssh
            cat node02.pub >> authorized_keys

    3）zookeeper集群搭建     部署在node02,node03,node04
        node02:
        tar -xf zookeeper-3.4.6.tar.gz
        mv zookeeper-3.4.6 /opt/bigdata
        cd /opt/bigdata/zookeeper-3.4.6
        cd conf
        cp zoo_sample.cfg zoo.cfg
        vi zoo.cfg
        dataDir=/var/bigdata/hadoop/zk
        server.1=node02:2888:3888
        server.2=node03:2888:3888
        server.3=node04:2888:3888

        cd /var/bigdata/hadoop/zk
        echo 1 > myid
        vi /etc/profile
        export ZOOKEEPER_HOME=/opt/bigdata/zookeeper-3.4.6
        export PATH=$PATH:xxx:$ZOOKEEPER_HOME/bin
        source /etc/profile

        cd /opt/bigdata
        scp -r ./bigdata-3.4.6/ node03:`pwd`
        scp -r ./bigdata-3.4.6/ node04:`pwd`
        修改node02和node03的myid，并且记得修改/etc/profile环境变量，让zk环境变量加进去

        启动zk：
        zkServer.sh start


使用验证：
    1）去看jn的日志和目录变化；
    2）node04
        zkCli.sh
            ls /
            启动之后可以看到锁：
            get /hadoop-ha/mycluster/ActiveStandbyElectorLock